{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome \u00b6 You have reached the website for the computational light laboratory. The computational light laboratory is part of Computer Science Department at University College London . The computational light laboratory is lead by Kaan Ak\u015fit . The computational light laboratory conducts research and development in light sciences, including computer-generated holography, computer graphics, computational imaging, computational displays and visual perception. Our primary software toolkit to tackle our research problems is public and open-source. We host our toolkit as Odak in GitHub. We translate our scientific output to actual lecture modules , and we create useful documentation for our research and development. To read more about our recent activities, please visit our recent timeline . Info Computational light laboratory is located at 169 Euston Road, London NW1 2AE, United Kingdom . Info Our laboratory organizes weekly seminars. To get more information about these seminars, please click here . Question If you are interested in joining our group as an intern, an undergraduate student, a master student, a PhD student, a postdoctoral researher or a visiting researcher, please do not hesitate to reach out to Kaan Ak\u015fit .","title":"Home"},{"location":"#welcome","text":"You have reached the website for the computational light laboratory. The computational light laboratory is part of Computer Science Department at University College London . The computational light laboratory is lead by Kaan Ak\u015fit . The computational light laboratory conducts research and development in light sciences, including computer-generated holography, computer graphics, computational imaging, computational displays and visual perception. Our primary software toolkit to tackle our research problems is public and open-source. We host our toolkit as Odak in GitHub. We translate our scientific output to actual lecture modules , and we create useful documentation for our research and development. To read more about our recent activities, please visit our recent timeline . Info Computational light laboratory is located at 169 Euston Road, London NW1 2AE, United Kingdom . Info Our laboratory organizes weekly seminars. To get more information about these seminars, please click here . Question If you are interested in joining our group as an intern, an undergraduate student, a master student, a PhD student, a postdoctoral researher or a visiting researcher, please do not hesitate to reach out to Kaan Ak\u015fit .","title":"Welcome"},{"location":"documentation/","text":"Documentation \u00b6 This page provides links to a list of documents. These documents vary in their topics, from workplace guidances to technical digests. For newcomers \u00b6 Documents Description Establishing yourself as a member This documentation is designed to help newcomers to establish themselves as a member of the Computational light laboratory. Learn more about computer-generated holography This documentation will link you to Odak's documentation on computer-generated holography. It is an excellent place to get ideas on how to get started on computer-generated holography. Explore the completed projects This link will get you to the list of completed projects. It can be a good resource to inspire your next and help you get aligned with the rest of the team. Logo of our team This documentation will describe to you the meaning of our team's logo, and it is also a place to download the source file of our logo.","title":"List"},{"location":"documentation/#documentation","text":"This page provides links to a list of documents. These documents vary in their topics, from workplace guidances to technical digests.","title":"Documentation"},{"location":"documentation/#for-newcomers","text":"Documents Description Establishing yourself as a member This documentation is designed to help newcomers to establish themselves as a member of the Computational light laboratory. Learn more about computer-generated holography This documentation will link you to Odak's documentation on computer-generated holography. It is an excellent place to get ideas on how to get started on computer-generated holography. Explore the completed projects This link will get you to the list of completed projects. It can be a good resource to inspire your next and help you get aligned with the rest of the team. Logo of our team This documentation will describe to you the meaning of our team's logo, and it is also a place to download the source file of our logo.","title":"For newcomers"},{"location":"documentation/getting_started/","text":"Welcome aboard! \u00b6 Quote Welcome aboard! The computational light laboratory conducts research and development in light sciences, including computer-generated holography, computer graphics, computational imaging, computational displays and visual perception. Our core mission is to show our societies that there can be better services, experiences, and goods that serve the benefits of humanity by using light. We are here to invent the next in light-based techniques and unlock the mystery of light. We build our tools to perform work and invent new methods to improve state of the art. Most importantly, we document our steps so that the others can follow. Finally, we release our work to the public on our GitHub organization . We have multiple social media outlets to promote our work. These include our Twitter account , our LinkedIn account, our YouTube account and our webpage . We don't shy away from going public and participate in public demonstrations with our prototypes. I wholeheartedly welcome every member at every stage to the Computational light laboratory. We can improve the state of the world, and I need your help in doing that! Kaan Ak\u015fit Getting aboard! \u00b6 In the rest of this documentation, you will find a checklist that will help you establish yourself as a member of the Computational light laboratory. There is also an additional subsection that provides a list of suggestions to help you get you to establish collaborative work ethics. Note that this and the other documents that you will find on this website are always subject to change. In fact, as a member, please do not hesitate to suggest improvements and be the change by actually having a pull request in the source repository . Checklist \u00b6 Are you full registered for the graduate programme? Is all the administrator work done? Relevant contact: cs.phdadmissions@ucl.ac.uk . Do you know when you will receive your first paycheck? Relevant contact: cs.phdadmissions@ucl.ac.uk . Do you have a UCL identity card? Relevant contact: securitysystems@ucl.ac.uk . Do you know which building is our office building? Reach out to Kaan or any other member and ask, k.aksit@ucl.ac.uk . Can you get into the building where our office is using your UCL identity card? Relevant contact: facilities@cs.ucl.ac.uk . Do you have a desk and a chair reserved for you in the office? Relevant contact: facilities@cs.ucl.ac.uk . Do you know where Kaan's office is? Reach out to Kaan or any other member and ask, k.aksit@ucl.ac.uk . Do you know where our laboratory space is? Reach out to Kaan or any other member and ask, k.aksit@ucl.ac.uk . Do you have a computer to conduct your research? Reach out to Kaan and ask, k.aksit@ucl.ac.uk . For queries such as where to get a display mount, cable for this and that, try reaching out to facilities@cs.ucl.ac.uk . Do you have access to remote computational resources that you may be needing in your work? Reach out to Kaan and ask, k.aksit@ucl.ac.uk . Do you have access to hardware resources that you may be needing in your work? Reach out to Kaan and ask, k.aksit@ucl.ac.uk . Make sure to meet other members. Send emails! They are listed on this website. . Ask about their experiences and thoughts. Explore what they are conducting in their research. Make sure to discuss with Kaan Ak\u015fit to see how you can contribute to Odak in the near future. Do you know what you will be focusing on? Do you know what projects are carried out in the team? Are you up-to-date with what the team has achieved recently ? Are you listed as a member in the GitHub organization ? In that organization which team do you belong to? Reach out to Kaan and ask, k.aksit@ucl.ac.uk . Do you have a weekly 1:1 meeting arranged with Kaan? Reach out to Kaan and ask, k.aksit@ucl.ac.uk . Are you receiving calendar invitations for weekly group meetings? Reach out to Kaan or any other team member and ask, k.aksit@ucl.ac.uk . Do you have a research plan? What are your goals? How will your research impact the society in the near and far future? Tinker deeply in a structured manner. Agree with Kaan and your other supervisors. Reach out to them and initiate conversations about your roadmap towards your degree. Suggestions \u00b6 These are some suggestions to help you get establishing yourself as a collaborative member of the group. Install software that helps you send emails. With that software, make sure you can schedule emails. Please do not send emails to people you don't know well outside 8 am to 6 pm (unless they are in a different time zone). Life brings many challenges, and not all days are sunny. Even if communication degrades over time, keep the kindness. Control yourself. Never say anything that you will regret! (Life is not war) We are all collaborators. The best things happen when people collaborate. Being a Swiss knife is good, but there isn't a leader in history that leads no one. There was no human on this planet can exist by themself. Avoid unnecessary communication, leave others room to organize themselves. If you are angry, stand up and walk. Take a break, be somewhere else for some time. When it is time, and you are calm, come back. Smile, stand up, walk, be kind and love yourself, and respect yourself. Know yourself! Spend time to understand things if you want to be an expert in the topic. Do not worry about how much it takes, but worry if you don't understand. You will be exposed to noise most of the time in your communications. Improve your filters to extract useful information. Do it now if you can. Tomorrow will arrive with new tasks. The work is not complete until it is complete. Don't be handwavy. Ensure that you provide a working solution (not an \"it can work easily in the next step\" solution). Research impact means the beneficial application of expertise, knowledge, analysis or discovery. It can also be described as an effect on change or benefit to the economy, society, culture, public policy or services, health, the environment or quality of life beyond academia. Build it. They will come.","title":"Welcome aboard!"},{"location":"documentation/getting_started/#welcome-aboard","text":"Quote Welcome aboard! The computational light laboratory conducts research and development in light sciences, including computer-generated holography, computer graphics, computational imaging, computational displays and visual perception. Our core mission is to show our societies that there can be better services, experiences, and goods that serve the benefits of humanity by using light. We are here to invent the next in light-based techniques and unlock the mystery of light. We build our tools to perform work and invent new methods to improve state of the art. Most importantly, we document our steps so that the others can follow. Finally, we release our work to the public on our GitHub organization . We have multiple social media outlets to promote our work. These include our Twitter account , our LinkedIn account, our YouTube account and our webpage . We don't shy away from going public and participate in public demonstrations with our prototypes. I wholeheartedly welcome every member at every stage to the Computational light laboratory. We can improve the state of the world, and I need your help in doing that! Kaan Ak\u015fit","title":"Welcome aboard!"},{"location":"documentation/getting_started/#getting-aboard","text":"In the rest of this documentation, you will find a checklist that will help you establish yourself as a member of the Computational light laboratory. There is also an additional subsection that provides a list of suggestions to help you get you to establish collaborative work ethics. Note that this and the other documents that you will find on this website are always subject to change. In fact, as a member, please do not hesitate to suggest improvements and be the change by actually having a pull request in the source repository .","title":"Getting aboard!"},{"location":"documentation/getting_started/#checklist","text":"Are you full registered for the graduate programme? Is all the administrator work done? Relevant contact: cs.phdadmissions@ucl.ac.uk . Do you know when you will receive your first paycheck? Relevant contact: cs.phdadmissions@ucl.ac.uk . Do you have a UCL identity card? Relevant contact: securitysystems@ucl.ac.uk . Do you know which building is our office building? Reach out to Kaan or any other member and ask, k.aksit@ucl.ac.uk . Can you get into the building where our office is using your UCL identity card? Relevant contact: facilities@cs.ucl.ac.uk . Do you have a desk and a chair reserved for you in the office? Relevant contact: facilities@cs.ucl.ac.uk . Do you know where Kaan's office is? Reach out to Kaan or any other member and ask, k.aksit@ucl.ac.uk . Do you know where our laboratory space is? Reach out to Kaan or any other member and ask, k.aksit@ucl.ac.uk . Do you have a computer to conduct your research? Reach out to Kaan and ask, k.aksit@ucl.ac.uk . For queries such as where to get a display mount, cable for this and that, try reaching out to facilities@cs.ucl.ac.uk . Do you have access to remote computational resources that you may be needing in your work? Reach out to Kaan and ask, k.aksit@ucl.ac.uk . Do you have access to hardware resources that you may be needing in your work? Reach out to Kaan and ask, k.aksit@ucl.ac.uk . Make sure to meet other members. Send emails! They are listed on this website. . Ask about their experiences and thoughts. Explore what they are conducting in their research. Make sure to discuss with Kaan Ak\u015fit to see how you can contribute to Odak in the near future. Do you know what you will be focusing on? Do you know what projects are carried out in the team? Are you up-to-date with what the team has achieved recently ? Are you listed as a member in the GitHub organization ? In that organization which team do you belong to? Reach out to Kaan and ask, k.aksit@ucl.ac.uk . Do you have a weekly 1:1 meeting arranged with Kaan? Reach out to Kaan and ask, k.aksit@ucl.ac.uk . Are you receiving calendar invitations for weekly group meetings? Reach out to Kaan or any other team member and ask, k.aksit@ucl.ac.uk . Do you have a research plan? What are your goals? How will your research impact the society in the near and far future? Tinker deeply in a structured manner. Agree with Kaan and your other supervisors. Reach out to them and initiate conversations about your roadmap towards your degree.","title":"Checklist"},{"location":"documentation/getting_started/#suggestions","text":"These are some suggestions to help you get establishing yourself as a collaborative member of the group. Install software that helps you send emails. With that software, make sure you can schedule emails. Please do not send emails to people you don't know well outside 8 am to 6 pm (unless they are in a different time zone). Life brings many challenges, and not all days are sunny. Even if communication degrades over time, keep the kindness. Control yourself. Never say anything that you will regret! (Life is not war) We are all collaborators. The best things happen when people collaborate. Being a Swiss knife is good, but there isn't a leader in history that leads no one. There was no human on this planet can exist by themself. Avoid unnecessary communication, leave others room to organize themselves. If you are angry, stand up and walk. Take a break, be somewhere else for some time. When it is time, and you are calm, come back. Smile, stand up, walk, be kind and love yourself, and respect yourself. Know yourself! Spend time to understand things if you want to be an expert in the topic. Do not worry about how much it takes, but worry if you don't understand. You will be exposed to noise most of the time in your communications. Improve your filters to extract useful information. Do it now if you can. Tomorrow will arrive with new tasks. The work is not complete until it is complete. Don't be handwavy. Ensure that you provide a working solution (not an \"it can work easily in the next step\" solution). Research impact means the beneficial application of expertise, knowledge, analysis or discovery. It can also be described as an effect on change or benefit to the economy, society, culture, public policy or services, health, the environment or quality of life beyond academia. Build it. They will come.","title":"Suggestions"},{"location":"documentation/logo/","text":"","title":"Our laboratory's logo"},{"location":"people/","text":"People \u00b6 This page will give you a complete of our current members. At the end of the page, you will also find our alumni list as a separate section. Current members \u00b6 All our current members are located in 169 Euston Road, London NW1 2AE, United Kingdom . Faculty \u00b6 Kaan Ak\u015fit Associate Professor E-mail Office: 409 Doctoral Students \u00b6 Oliver Kingshott PhD student (Second supervisor: Tobias Ritschel ) E-mail Office: Alumni \u00b6 Master Students \u00b6 2021 \u00b6 Oliver Kingshott, Learned Point-spread Functions for Lensless Imaging . Koray Kavakl\u0131, Towards Improving Visual Quality in Computer-Generated Holography . Chengkun Li, Neural Optical Beam Propagation . Yuze Yang, Learned 3D Representations: Point Cloud, Depth Maps and Holograms .","title":"Current members and alumni"},{"location":"people/#people","text":"This page will give you a complete of our current members. At the end of the page, you will also find our alumni list as a separate section.","title":"People"},{"location":"people/#current-members","text":"All our current members are located in 169 Euston Road, London NW1 2AE, United Kingdom .","title":"Current members"},{"location":"people/#faculty","text":"Kaan Ak\u015fit Associate Professor E-mail Office: 409","title":"Faculty"},{"location":"people/#doctoral-students","text":"Oliver Kingshott PhD student (Second supervisor: Tobias Ritschel ) E-mail Office:","title":"Doctoral Students"},{"location":"people/#alumni","text":"","title":"Alumni"},{"location":"people/#master-students","text":"","title":"Master Students"},{"location":"people/#2021","text":"Oliver Kingshott, Learned Point-spread Functions for Lensless Imaging . Koray Kavakl\u0131, Towards Improving Visual Quality in Computer-Generated Holography . Chengkun Li, Neural Optical Beam Propagation . Yuze Yang, Learned 3D Representations: Point Cloud, Depth Maps and Holograms .","title":"2021"},{"location":"publications/","text":"Publications \u00b6 2021 \u00b6 Telelife: The Future of Remote Living Jason Orlosky , Misha Sra , Kenan Bekta\u015f , Huaishu Peng , Jeeeun Kim , Nataliya Kosmyna , Tobias Hollerer , Anthony Steed , Kiyoshi Kiyokawa , and Kaan Ak\u015fit Manuscript Learned Holographic Light Transport Koray Kavakl\u0131, Hakan Urey and Kaan Ak\u015fit Project site Manuscript Code SensiCut: Material-Aware Laser Cutting Using Speckle Sensing and Deep Learning Mustafa Doga Dogan , Steven Vidal Acevedo Colon, Varnika Sinha, Kaan Ak\u015fit and Stefanie Mueller Project site Manuscript Project video Presentation recording Beyond Blur: Ventral Metamers for Foveated Rendering David R. Walton , Rafael Kuffner Dos Anjos, Sebastian Friston, David Swapp, Kaan Ak\u015fit , Anthony Steed and Tobias Ritschel Publisher site Project site Manuscript Bibtex @article{walton2021beyond, author = {David R. Walton and Rafael Kuffner Dos Anjos and Sebastian Friston and David Swapp and Kaan Ak\u015fit and Anthony Steed and Tobias Ritschel}, title = {Beyond Blur: Ventral Metamers for Foveated Rendering}, journal = {ACM Trans. Graph. (Proc. SIGGRAPH 2021)}, year = {2021}, volume = {40}, number = {4}, } Beaming Displays Yuta Itoh , Takumi Kaminokado and Kaan Ak\u015fit Publisher site Project site Manuscript Project video Presentation recording Bibtex @article{itoh2021beaming, author = {Yuta Itoh, Takumi Kaminokado, and Kaan Ak{s}it}, keywords = {Near-eye displays}, publisher = {IEEE VR}, title = {Beaming Displays}, month = {April}, year = {2021} } 2020 \u00b6 Optical Gaze Tracking with Spatially-Sparse Single-Pixel Detectors Richard Li , Eric Whitmire , Michael Stengel , Ben Boudaoud, Jan Kautz David Luebke , Shwetak Patel and Kaan Ak\u015fit Publisher site Project site Manuscript Presentation recording Bibtex @article{li2020opticalgaze, author = {Richard Li, Eric Whitmire, Michael Stengel, Ben Boudaoud, Jan Kautz, David Luebke, Shwetak Patel, and Kaan Ak{s}it}, keywords = {Gaze tracking, eye tracking, LEDs, photodiodes}, publisher = {ISMAR}, title = {Optical Gaze Tracking with Spatially-Sparse Single-Pixel Detectors}, month = {Nov}, year = {2020} } Patch scanning displays: spatiotemporal enhancement for displays Kaan Ak\u015fit Publisher site Project site Manuscript Project video Bibtex @article{aksit2020patch, author = {Kaan Ak\\c{s}it}, journal = {Opt. Express}, keywords = {Digital micromirror devices; Image quality; Image reconstruction; Light sources; Optical components; Three dimensional imaging}, number = {2}, pages = {2107--2121}, publisher = {OSA}, title = {Patch scanning displays: spatiotemporal enhancement for displays}, volume = {28}, month = {Jan}, year = {2020}, url = {http://www.opticsexpress.org/abstract.cfm?URI=oe-28-2-2107} } 2019 \u00b6 Near-Eye Display and Tracking Technologies for Virtual and Augmented Reality George Alex Koulieris Kaan Ak\u015fit , Michael Stengel , Rafa\u0142 Mantiuk , Katerina Mania and Christian Richardt Publisher site Project site Manuscript Project video Bibtex @article{NearEyeDisplayAndTrackingSTAR, author = {George Alex Koulieris and Kaan Ak{\\c{s}}it and Michael Stengel and Rafa{\\l} K. Mantiuk and Katerina Mania and Christian Richardt}, title = {Near-Eye Display and Tracking Technologies for Virtual and Augmented Reality}, journal = {Computer Graphics Forum}, year = {2019}, volume = {38}, number = {2}, url = {https://richardt.name/nedtt/}, } Foveated AR: Dynamically-Foveated Augmented Reality Display Emerging Technology best in show award at SIGGRAPH 2019 Jonghyun Kim , Youngmo Jeong , Michael Stengel , Kaan Ak\u015fit , Rachel Albert , Ben Boudaoud, Trey Greer, Joohwan Kim, Ward Lopes, Zander Majercik, Peter Shirley , Josef Spjut , Morgan Mcguire and David Luebke Publisher site Project site Manuscript Project video Bibtex @article{kim2019foveated, title={Foveated AR: dynamically-foveated augmented reality display}, author={Kim, Jonghyun and Jeong, Youngmo and Stengel, Michael and Ak{\\c{s}}it, Kaan and Albert, Rachel and Boudaoud, Ben and Greer, Trey and Kim, Joohwan and Lopes, Ward and Majercik, Zander and others}, journal={ACM Transactions on Graphics (TOG)}, volume={38}, number={4}, pages={1--15}, year={2019}, publisher={ACM New York, NY, USA} } 2018 \u00b6 FocusAR: Auto-focus Augmented Reality Eyeglasses for both Real and Virtual Best paper award at ISMAR 2018 Presented at SIGGRAPH ASIA 2018 Praneeth Chakravarthula , David Dunn , Kaan Ak\u015fit and Henry Fuchs Publisher site Project site Manuscript Presentation recording Presentation source Bibtex @article{chakravarthula2018focusar, title={focusar: auto-focus augmented reality eyeglasses for both real and virtual}, author={chakravarthula, praneeth and dunn, david and ak{\\c{s}}it, kaan and fuchs, henry}, journal={ieee transactions on visualization and computer graphics}, year={2018}, publisher={ieee} } Manufacturing Application-Driven Foveated Near-Eye Displays Best paper nominee at IEEE VR 2018 Emerging Technology best in show award at SIGGRAPH 2018 Kaan Ak\u015fit , Praneeth Chakravarthula , Kishore Rathinavel , Youngmo Jeong , Rachel Albert , Henry Fuchs and David Luebke Publisher site Project site Manuscript Project video Presentation recording Presentation source Bibtex @article{akcsit2019manufacturing, title={Manufacturing application-driven foveated near-eye displays}, author={Ak{\\c{s}}it, Kaan and Chakravarthula, Praneeth and Rathinavel, Kishore and Jeong, Youngmo and Albert, Rachel and Fuchs, Henry and Luebke, David}, journal={IEEE transactions on visualization and computer graphics}, volume={25}, number={5}, pages={1928--1939}, year={2019}, publisher={IEEE} } 2017 \u00b6 Near-Eye Varifocal Augmented Reality Display using See-Through Screens Kaan Ak\u015fit , Ward Lopes, Jonghyun Kim , Peter Shirley and David Luebke Publisher site Project site Manuscript Video Bibtex @Article{Aksit2017Varifocal, Title = {Near-Eye Varifocal Augmented Reality Display using See-Through Screens}, Author = {K. Ak{\\c{s}}it and W. Lopes and J. Kim and P. Shirley and D. Luebke}, journal = {ACM Trans. Graph. (SIGGRAPH)}, issue = {36}, number = {6}, year = {2017}} Wide Field Of View Varifocal Near-Eye Display Using See-Through Deformable Membrane Mirrors Best paper award at IEEE VR 2017 SIGGRAPH 2017 Emerging Technologies DCEXPO Special Prize David Dunn , Cary Tippets, Kent Torell, Petr Kellnhofer , Kaan Ak\u015fit , Piotr Didyk , Karol Myszkowski , D. Luebke , Henry Fuchs Publisher site Project site Manuscript Video Bibtex @article{dunn2017wide, title={Wide Field Of View Varifocal Near-Eye Display Using See-Through Deformable Membrane Mirrors}, author={Dunn, David and Tippets, Cary and Torell, Kent and Kellnhofer, Petr and Ak{\\c{s}}it, Kaan and Didyk, Piotr and Myszkowski, Karol and Luebke, David and Fuchs, Henry}, journal={IEEE Transactions on Visualization and Computer Graphics}, volume={23}, number={4}, pages={1322--1331}, year={2017}, publisher={IEEE} }} 2016 \u00b6 Slim near-eye display using pinhole aperture arrays Kaan Ak\u015fit , Jan Kautz and David Luebke Publisher site Project site Manuscript Video Bibtex @article{akcsit2020gaze, title={Gaze-sensing leds for head mounted displays}, author={Ak{\\c{s}}it, Kaan and Kautz, Jan and Luebke, David}, journal={arXiv preprint arXiv:2003.08499}, year={2020} } 2015 \u00b6 Slim near-eye display using pinhole aperture arrays Kaan Ak\u015fit , Jan Kautz and David Luebke Publisher site Project site Manuscript Video Bibtex @article{Aksit:15, author = {Kaan Ak\\c{s}it and Jan Kautz and David Luebke}, journal = {Appl. Opt.}, keywords = {Apertures; Vision - binocular and stereopsis ; Computational imaging}, number = {11}, pages = {3422--3427}, publisher = {OSA}, title = {Slim near-eye display using pinhole aperture arrays}, volume = {54}, month = {Apr}, year = {2015}, url = {http://ao.osa.org/abstract.cfm?URI=ao-54-11-3422}, doi = {10.1364/AO.54.003422}, abstract = {We report a new technique for building a wide-angle, lightweight, thin-form-factor, cost-effective, easy-to-manufacture near-eye head-mounted display (HMD) for virtual reality applications. Our approach adopts an aperture mask containing an array of pinholes and a screen as a source of imagery. We demonstrate proof-of-concept HMD prototypes with a binocular field of view (FOV) of 70\\&amp;\\#xB0;\\&amp;\\#xD7;45\\&amp;\\#xB0;, or total diagonal FOV of 83\\&amp;\\#xB0;. This FOV should increase with increasing display panel size. The optical angular resolution supported in our prototype can go down to 1.4\\&amp;\\#x2013;2.1 arcmin by adopting a display with 20\\&amp;\\#x2013;30\\&amp;\\#xA0;\\&amp;\\#x3BC;m pixel pitch.}, } 2014 \u00b6 Head-worn Mixed Reality Projection Display Application Kaan Ak\u015fit , Daniel Kade, O\u011fuzhan \u00d6zcan and Hakan Urey Publisher site Project site Manuscript Video Bibtex @inproceedings{Aksit:2014:HMR:2663806.2663826, author = {Ak\\c{s}it, Kaan and Kade, Daniel and \\\"{O}zcan, O\\u{g}uzhan and \\\"{U}rey, Hakan}, title = {Head-worn Mixed Reality Projection Display Application}, booktitle = {Proceedings of the 11th Conference on Advances in Computer Entertainment Technology}, series = {ACE '14}, year = {2014}, isbn = {978-1-4503-2945-3}, location = {Funchal, Portugal}, pages = {11:1--11:9}, articleno = {11}, numpages = {9}, url = {http://doi.acm.org/10.1145/2663806.2663826}, doi = {10.1145/2663806.2663826}, acmid = {2663826}, publisher = {ACM}, address = {New York, NY, USA}, keywords = {head-mounted projection display, immersive environments, laser projector, mixed reality, motion capture}, } Super stereoscopy technique for comfortable and realistic 3D displays Kaan Ak\u015fit , Amir Niaki, Erdem Ulusoy and Hakan Urey Publisher site Project site Manuscript Bibtex @article{Aksit:14, author = {Kaan Ak\\c{s}it and Amir Hossein Ghanbari Niaki and Erdem Ulusoy and Hakan Urey}, journal = {Opt. Lett.}, keywords = {Displays; Vision - binocular and stereopsis ; Visual optics, accommodation}, number = {24}, pages = {6903--6906}, publisher = {OSA}, title = {Super stereoscopy technique for comfortable and realistic 3D displays}, volume = {39}, month = {Dec}, year = {2014}, url = {http://ol.osa.org/abstract.cfm?URI=ol-39-24-6903}, doi = {10.1364/OL.39.006903}, abstract = {Two well-known problems of stereoscopic displays are the accommodation-convergence conflict and the lack of natural blur for defocused objects. We present a new technique that we name Super Stereoscopy (SS3D) to provide a convenient solution to these problems. Regular stereoscopic glasses are replaced by SS3D glasses which deliver at least two parallax images per eye through pinholes equipped with light selective filters. The pinholes generate blur-free retinal images so as to enable correct accommodation, while the delivery of multiple parallax images per eye creates an approximate blur effect for defocused objects. Experiments performed with cameras and human viewers indicate that the technique works as desired. In case two, pinholes equipped with color filters per eye are used; the technique can be used on a regular stereoscopic display by only uploading a new content, without requiring any change in display hardware, driver, or frame rate. Apart from some tolerable loss in display brightness and decrease in natural spatial resolution limit of the eye because of pinholes, the technique is quite promising for comfortable and realistic 3D vision, especially enabling the display of close objects that are not possible to display and comfortably view on regular 3DTV and cinema.}, } From Sound to Sight: Using Audio Processing to enable Visible Light Communication Stefan Schmid, D. Schwyn, Kaan Ak\u015fit , Giorgio Corbellini, Thomas Gross and Stefan Mangold Publisher site Project site Manuscript Bibtex @INPROCEEDINGS{7063484, author={S. Schmid and D. Schwyn and K. Ak\u015fit and G. Corbellini and T. R. Gross and S. Mangold}, booktitle={2014 IEEE Globecom Workshops (GC Wkshps)}, title={From sound to sight: Using audio processing to enable visible light communication}, year={2014}, pages={518-523}, keywords={audio signal processing;light emitting diodes;mobile handsets;optical communication;photodiodes;protocols;audio jack;audio processing;communication protocols;electrical signals;light signals;microphone input;mobile phones;on-board audio signal processing;passive components;peripheral device;photodiode;visible light communication;Decoding;Hardware;Lifting equipment;Light emitting diodes;Photodiodes;Protocols;Throughput}, doi={10.1109/GLOCOMW.2014.7063484}, ISSN={2166-0077}, month={Dec},} Connecting Networks of Toys and Smartphones with Visible Light Communication Giorgio Corbellini, Kaan Ak\u015fit , Stefan Mangold Stefan Schmid and Thomas R. Gross Publisher site Project site Manuscript Video Bibtex @ARTICLE{6852086, author={G. Corbellini and K. Aksit and S. Schmid and S. Mangold and T. R. Gross}, journal={IEEE Communications Magazine}, title={Connecting networks of toys and smartphones with visible light communication}, year={2014}, volume={52}, number={7}, pages={72-78}, keywords={light emitting diodes;optical communication;optical receivers;smart phones;LED;VLC systems;brightness;consumer electronics;illumination;light emitting diodes;light receivers;microcontrollers;public environment;residential environment;smartphones;toys;visible light communication;wireless communication interface;Cameras;Commercialization;Frequency measurement;Illumination;Light emitting diodes;Microcontrollers;Receivers;Smart phones;Transceivers}, doi={10.1109/MCOM.2014.6852086}, ISSN={0163-6804}, month={July},} 2013 \u00b6 Dynamic exit pupil trackers for autostereoscopic displays Kaan Ak\u015fit , Hadi Baghsiahi, P. Surman, Selim \u04e6l\u00e7er, E. Willman, David R. Selviah, Sally Day and Hakan Urey Publisher site Project site Manuscript Video Bibtex @article{Aksit:13, author = {Kaan Ak\\c{s}it and Hadi Baghsiahi and Phil Surman and Selim \u04e6l\\c{c}er and Eero Willman and David R. Selviah and Sally Day and Hakan Urey}, journal = {Opt. Express}, keywords = {Displays; Optical systems; Optoelectronics; Laser beam shaping; Vision - binocular and stereopsis}, number = {12}, pages = {14331--14341}, publisher = {OSA}, title = {Dynamic exit pupil trackers for autostereoscopic displays}, volume = {21}, month = {Jun}, year = {2013}, url = {http://www.opticsexpress.org/abstract.cfm?URI=oe-21-12-14331}, doi = {10.1364/OE.21.014331}, abstract = {This paper describes the first demonstrations of two dynamic exit pupil (DEP) tracker techniques for autostereoscopic displays. The first DEP tracker forms an exit pupil pair for a single viewer in a defined space with low intraocular crosstalk using a pair of moving shutter glasses located within the optical system. A display prototype using the first DEP tracker is constructed from a pair of laser projectors, pupil-forming optics, moving shutter glasses at an intermediate pupil plane, an image relay lens, and a Gabor superlens based viewing screen. The left and right eye images are presented time-sequentially to a single viewer and seen as a 3D image without wearing glasses and allows the viewer to move within a region of 40 cm {\\texttimes} 20 cm in the lateral plane, and 30 cm along the axial axis. The second DEP optics can move the exit pupil location dynamically in a much larger 3D space by using a custom spatial light modulator (SLM) forming an array of shutters. Simultaneous control of multiple exit pupils in both lateral and axial axes is demonstrated for the first time and provides a viewing volume with an axial extent of 0.6{\\textminus}3 m from the screen and within a lateral viewing angle of {\\textpm} 20{\\textdegree} for multiple viewers. This system has acceptable crosstalk (\\&lt; 5\\%) between the stereo image pairs. In this novel version of the display the optical system is used as an advanced dynamic backlight for a liquid crystal display (LCD). This has advantages in terms of overall display size as there is no requirement for an intermediate image, and in image quality. This system has acceptable crosstalk (\\&lt; 5\\%) between the stereo image pairs.}, } Multi-view autostereoscopic projection display using rotating screen Spotlight on Optics Osman Eldes, Kaan Ak\u015fit and Hakan Urey Publisher site Project site Manuscript Video Bibtex @article{Eldes:13, author = {Osman Eldes and Kaan Ak\\c{s}it and Hakan Urey}, journal = {Opt. Express}, keywords = {Displays; Diffusers; Vision - binocular and stereopsis ; Autostereoscopic displays; Brightness; Fresnel lenses; Image registration; Pico projectors; Systems design}, number = {23}, pages = {29043--29054}, publisher = {OSA}, title = {Multi-view autostereoscopic projection display using rotating screen}, volume = {21}, month = {Nov}, year = {2013}, url = {http://www.osapublishing.org/oe/abstract.cfm?URI=oe-21-23-29043}, doi = {10.1364/OE.21.029043}, abstract = {A new technique for multi-view autostereoscopic projection display is proposed, and demonstrated. The technique uses two mobile projectors, a rotating retro-reflective diffuser screen, and a head-tracking camera. As two dynamic viewing slits are created at the viewer's position, the slits can track the position of the eyes by rotating the screen. The display allows a viewer to move approximately 700 mm along the horizontal axis, and 500 mm along the vertical axis with an average crosstalk below 5 \\%. Two screen prototypes with different diffusers have been tried, and they provide luminance levels of 60 Cd/m2, and 160 Cd/m2 within the viewing field.}, } 2012 \u00b6 Portable 3D Laser Projector Using Mixed Polarization Technique Best 3D product award of International 3D Society (4th year) Kaan Ak\u015fit , Osman Elde\u015f, Selvan Viswanathan, Mark Freeman and Hakan Urey Publisher site Project site Manuscript Video Bibtex @ARTICLE{6297485, author={Aksit, Kaan and Eldes, Osman and Viswanathan, Selvan and Freeman, Mark O. and Urey, Hakan}, journal={Journal of Display Technology}, title={Portable 3D Laser Projector Using Mixed Polarization Technique}, year={2012}, volume={8}, number={10}, pages={582-589}, doi={10.1109/JDT.2012.2205664}} 2010 \u00b6 Heart rate monitoring via remote photoplethysmography with motion artifacts reduction Giovanni Cennini, Jeremie Arguel, Kaan Ak\u015fit and Arno van Leest Publisher site Project site Manuscript Video Bibtex @article{Cennini:10, author = {Giovanni Cennini and Jeremie Arguel and Kaan Ak\\c{s}it and Arno van Leest}, journal = {Opt. Express}, keywords = {Medical optics instrumentation; Optical devices; Optical sensing and sensors}, number = {5}, pages = {4867--4875}, publisher = {OSA}, title = {Heart rate monitoring via remote photoplethysmography with motion artifacts reduction}, volume = {18}, month = {Mar}, year = {2010}, url = {http://www.opticsexpress.org/abstract.cfm?URI=oe-18-5-4867}, doi = {10.1364/OE.18.004867}, abstract = {In this paper, we present a novel photoplethysmographic device that operates remotely, i.e. not in contact with the skin. The device allows for real time measurements of heart rate with motion artifact reduction from a distance of a few centimeters up to several meters. High mobility of users is achieved in assessment of vital body signs, such as heart rate.}, }","title":"List of publications"},{"location":"publications/#publications","text":"","title":"Publications"},{"location":"publications/#2021","text":"Telelife: The Future of Remote Living Jason Orlosky , Misha Sra , Kenan Bekta\u015f , Huaishu Peng , Jeeeun Kim , Nataliya Kosmyna , Tobias Hollerer , Anthony Steed , Kiyoshi Kiyokawa , and Kaan Ak\u015fit Manuscript Learned Holographic Light Transport Koray Kavakl\u0131, Hakan Urey and Kaan Ak\u015fit Project site Manuscript Code SensiCut: Material-Aware Laser Cutting Using Speckle Sensing and Deep Learning Mustafa Doga Dogan , Steven Vidal Acevedo Colon, Varnika Sinha, Kaan Ak\u015fit and Stefanie Mueller Project site Manuscript Project video Presentation recording Beyond Blur: Ventral Metamers for Foveated Rendering David R. Walton , Rafael Kuffner Dos Anjos, Sebastian Friston, David Swapp, Kaan Ak\u015fit , Anthony Steed and Tobias Ritschel Publisher site Project site Manuscript Bibtex @article{walton2021beyond, author = {David R. Walton and Rafael Kuffner Dos Anjos and Sebastian Friston and David Swapp and Kaan Ak\u015fit and Anthony Steed and Tobias Ritschel}, title = {Beyond Blur: Ventral Metamers for Foveated Rendering}, journal = {ACM Trans. Graph. (Proc. SIGGRAPH 2021)}, year = {2021}, volume = {40}, number = {4}, } Beaming Displays Yuta Itoh , Takumi Kaminokado and Kaan Ak\u015fit Publisher site Project site Manuscript Project video Presentation recording Bibtex @article{itoh2021beaming, author = {Yuta Itoh, Takumi Kaminokado, and Kaan Ak{s}it}, keywords = {Near-eye displays}, publisher = {IEEE VR}, title = {Beaming Displays}, month = {April}, year = {2021} }","title":"2021"},{"location":"publications/#2020","text":"Optical Gaze Tracking with Spatially-Sparse Single-Pixel Detectors Richard Li , Eric Whitmire , Michael Stengel , Ben Boudaoud, Jan Kautz David Luebke , Shwetak Patel and Kaan Ak\u015fit Publisher site Project site Manuscript Presentation recording Bibtex @article{li2020opticalgaze, author = {Richard Li, Eric Whitmire, Michael Stengel, Ben Boudaoud, Jan Kautz, David Luebke, Shwetak Patel, and Kaan Ak{s}it}, keywords = {Gaze tracking, eye tracking, LEDs, photodiodes}, publisher = {ISMAR}, title = {Optical Gaze Tracking with Spatially-Sparse Single-Pixel Detectors}, month = {Nov}, year = {2020} } Patch scanning displays: spatiotemporal enhancement for displays Kaan Ak\u015fit Publisher site Project site Manuscript Project video Bibtex @article{aksit2020patch, author = {Kaan Ak\\c{s}it}, journal = {Opt. Express}, keywords = {Digital micromirror devices; Image quality; Image reconstruction; Light sources; Optical components; Three dimensional imaging}, number = {2}, pages = {2107--2121}, publisher = {OSA}, title = {Patch scanning displays: spatiotemporal enhancement for displays}, volume = {28}, month = {Jan}, year = {2020}, url = {http://www.opticsexpress.org/abstract.cfm?URI=oe-28-2-2107} }","title":"2020"},{"location":"publications/#2019","text":"Near-Eye Display and Tracking Technologies for Virtual and Augmented Reality George Alex Koulieris Kaan Ak\u015fit , Michael Stengel , Rafa\u0142 Mantiuk , Katerina Mania and Christian Richardt Publisher site Project site Manuscript Project video Bibtex @article{NearEyeDisplayAndTrackingSTAR, author = {George Alex Koulieris and Kaan Ak{\\c{s}}it and Michael Stengel and Rafa{\\l} K. Mantiuk and Katerina Mania and Christian Richardt}, title = {Near-Eye Display and Tracking Technologies for Virtual and Augmented Reality}, journal = {Computer Graphics Forum}, year = {2019}, volume = {38}, number = {2}, url = {https://richardt.name/nedtt/}, } Foveated AR: Dynamically-Foveated Augmented Reality Display Emerging Technology best in show award at SIGGRAPH 2019 Jonghyun Kim , Youngmo Jeong , Michael Stengel , Kaan Ak\u015fit , Rachel Albert , Ben Boudaoud, Trey Greer, Joohwan Kim, Ward Lopes, Zander Majercik, Peter Shirley , Josef Spjut , Morgan Mcguire and David Luebke Publisher site Project site Manuscript Project video Bibtex @article{kim2019foveated, title={Foveated AR: dynamically-foveated augmented reality display}, author={Kim, Jonghyun and Jeong, Youngmo and Stengel, Michael and Ak{\\c{s}}it, Kaan and Albert, Rachel and Boudaoud, Ben and Greer, Trey and Kim, Joohwan and Lopes, Ward and Majercik, Zander and others}, journal={ACM Transactions on Graphics (TOG)}, volume={38}, number={4}, pages={1--15}, year={2019}, publisher={ACM New York, NY, USA} }","title":"2019"},{"location":"publications/#2018","text":"FocusAR: Auto-focus Augmented Reality Eyeglasses for both Real and Virtual Best paper award at ISMAR 2018 Presented at SIGGRAPH ASIA 2018 Praneeth Chakravarthula , David Dunn , Kaan Ak\u015fit and Henry Fuchs Publisher site Project site Manuscript Presentation recording Presentation source Bibtex @article{chakravarthula2018focusar, title={focusar: auto-focus augmented reality eyeglasses for both real and virtual}, author={chakravarthula, praneeth and dunn, david and ak{\\c{s}}it, kaan and fuchs, henry}, journal={ieee transactions on visualization and computer graphics}, year={2018}, publisher={ieee} } Manufacturing Application-Driven Foveated Near-Eye Displays Best paper nominee at IEEE VR 2018 Emerging Technology best in show award at SIGGRAPH 2018 Kaan Ak\u015fit , Praneeth Chakravarthula , Kishore Rathinavel , Youngmo Jeong , Rachel Albert , Henry Fuchs and David Luebke Publisher site Project site Manuscript Project video Presentation recording Presentation source Bibtex @article{akcsit2019manufacturing, title={Manufacturing application-driven foveated near-eye displays}, author={Ak{\\c{s}}it, Kaan and Chakravarthula, Praneeth and Rathinavel, Kishore and Jeong, Youngmo and Albert, Rachel and Fuchs, Henry and Luebke, David}, journal={IEEE transactions on visualization and computer graphics}, volume={25}, number={5}, pages={1928--1939}, year={2019}, publisher={IEEE} }","title":"2018"},{"location":"publications/#2017","text":"Near-Eye Varifocal Augmented Reality Display using See-Through Screens Kaan Ak\u015fit , Ward Lopes, Jonghyun Kim , Peter Shirley and David Luebke Publisher site Project site Manuscript Video Bibtex @Article{Aksit2017Varifocal, Title = {Near-Eye Varifocal Augmented Reality Display using See-Through Screens}, Author = {K. Ak{\\c{s}}it and W. Lopes and J. Kim and P. Shirley and D. Luebke}, journal = {ACM Trans. Graph. (SIGGRAPH)}, issue = {36}, number = {6}, year = {2017}} Wide Field Of View Varifocal Near-Eye Display Using See-Through Deformable Membrane Mirrors Best paper award at IEEE VR 2017 SIGGRAPH 2017 Emerging Technologies DCEXPO Special Prize David Dunn , Cary Tippets, Kent Torell, Petr Kellnhofer , Kaan Ak\u015fit , Piotr Didyk , Karol Myszkowski , D. Luebke , Henry Fuchs Publisher site Project site Manuscript Video Bibtex @article{dunn2017wide, title={Wide Field Of View Varifocal Near-Eye Display Using See-Through Deformable Membrane Mirrors}, author={Dunn, David and Tippets, Cary and Torell, Kent and Kellnhofer, Petr and Ak{\\c{s}}it, Kaan and Didyk, Piotr and Myszkowski, Karol and Luebke, David and Fuchs, Henry}, journal={IEEE Transactions on Visualization and Computer Graphics}, volume={23}, number={4}, pages={1322--1331}, year={2017}, publisher={IEEE} }}","title":"2017"},{"location":"publications/#2016","text":"Slim near-eye display using pinhole aperture arrays Kaan Ak\u015fit , Jan Kautz and David Luebke Publisher site Project site Manuscript Video Bibtex @article{akcsit2020gaze, title={Gaze-sensing leds for head mounted displays}, author={Ak{\\c{s}}it, Kaan and Kautz, Jan and Luebke, David}, journal={arXiv preprint arXiv:2003.08499}, year={2020} }","title":"2016"},{"location":"publications/#2015","text":"Slim near-eye display using pinhole aperture arrays Kaan Ak\u015fit , Jan Kautz and David Luebke Publisher site Project site Manuscript Video Bibtex @article{Aksit:15, author = {Kaan Ak\\c{s}it and Jan Kautz and David Luebke}, journal = {Appl. Opt.}, keywords = {Apertures; Vision - binocular and stereopsis ; Computational imaging}, number = {11}, pages = {3422--3427}, publisher = {OSA}, title = {Slim near-eye display using pinhole aperture arrays}, volume = {54}, month = {Apr}, year = {2015}, url = {http://ao.osa.org/abstract.cfm?URI=ao-54-11-3422}, doi = {10.1364/AO.54.003422}, abstract = {We report a new technique for building a wide-angle, lightweight, thin-form-factor, cost-effective, easy-to-manufacture near-eye head-mounted display (HMD) for virtual reality applications. Our approach adopts an aperture mask containing an array of pinholes and a screen as a source of imagery. We demonstrate proof-of-concept HMD prototypes with a binocular field of view (FOV) of 70\\&amp;\\#xB0;\\&amp;\\#xD7;45\\&amp;\\#xB0;, or total diagonal FOV of 83\\&amp;\\#xB0;. This FOV should increase with increasing display panel size. The optical angular resolution supported in our prototype can go down to 1.4\\&amp;\\#x2013;2.1 arcmin by adopting a display with 20\\&amp;\\#x2013;30\\&amp;\\#xA0;\\&amp;\\#x3BC;m pixel pitch.}, }","title":"2015"},{"location":"publications/#2014","text":"Head-worn Mixed Reality Projection Display Application Kaan Ak\u015fit , Daniel Kade, O\u011fuzhan \u00d6zcan and Hakan Urey Publisher site Project site Manuscript Video Bibtex @inproceedings{Aksit:2014:HMR:2663806.2663826, author = {Ak\\c{s}it, Kaan and Kade, Daniel and \\\"{O}zcan, O\\u{g}uzhan and \\\"{U}rey, Hakan}, title = {Head-worn Mixed Reality Projection Display Application}, booktitle = {Proceedings of the 11th Conference on Advances in Computer Entertainment Technology}, series = {ACE '14}, year = {2014}, isbn = {978-1-4503-2945-3}, location = {Funchal, Portugal}, pages = {11:1--11:9}, articleno = {11}, numpages = {9}, url = {http://doi.acm.org/10.1145/2663806.2663826}, doi = {10.1145/2663806.2663826}, acmid = {2663826}, publisher = {ACM}, address = {New York, NY, USA}, keywords = {head-mounted projection display, immersive environments, laser projector, mixed reality, motion capture}, } Super stereoscopy technique for comfortable and realistic 3D displays Kaan Ak\u015fit , Amir Niaki, Erdem Ulusoy and Hakan Urey Publisher site Project site Manuscript Bibtex @article{Aksit:14, author = {Kaan Ak\\c{s}it and Amir Hossein Ghanbari Niaki and Erdem Ulusoy and Hakan Urey}, journal = {Opt. Lett.}, keywords = {Displays; Vision - binocular and stereopsis ; Visual optics, accommodation}, number = {24}, pages = {6903--6906}, publisher = {OSA}, title = {Super stereoscopy technique for comfortable and realistic 3D displays}, volume = {39}, month = {Dec}, year = {2014}, url = {http://ol.osa.org/abstract.cfm?URI=ol-39-24-6903}, doi = {10.1364/OL.39.006903}, abstract = {Two well-known problems of stereoscopic displays are the accommodation-convergence conflict and the lack of natural blur for defocused objects. We present a new technique that we name Super Stereoscopy (SS3D) to provide a convenient solution to these problems. Regular stereoscopic glasses are replaced by SS3D glasses which deliver at least two parallax images per eye through pinholes equipped with light selective filters. The pinholes generate blur-free retinal images so as to enable correct accommodation, while the delivery of multiple parallax images per eye creates an approximate blur effect for defocused objects. Experiments performed with cameras and human viewers indicate that the technique works as desired. In case two, pinholes equipped with color filters per eye are used; the technique can be used on a regular stereoscopic display by only uploading a new content, without requiring any change in display hardware, driver, or frame rate. Apart from some tolerable loss in display brightness and decrease in natural spatial resolution limit of the eye because of pinholes, the technique is quite promising for comfortable and realistic 3D vision, especially enabling the display of close objects that are not possible to display and comfortably view on regular 3DTV and cinema.}, } From Sound to Sight: Using Audio Processing to enable Visible Light Communication Stefan Schmid, D. Schwyn, Kaan Ak\u015fit , Giorgio Corbellini, Thomas Gross and Stefan Mangold Publisher site Project site Manuscript Bibtex @INPROCEEDINGS{7063484, author={S. Schmid and D. Schwyn and K. Ak\u015fit and G. Corbellini and T. R. Gross and S. Mangold}, booktitle={2014 IEEE Globecom Workshops (GC Wkshps)}, title={From sound to sight: Using audio processing to enable visible light communication}, year={2014}, pages={518-523}, keywords={audio signal processing;light emitting diodes;mobile handsets;optical communication;photodiodes;protocols;audio jack;audio processing;communication protocols;electrical signals;light signals;microphone input;mobile phones;on-board audio signal processing;passive components;peripheral device;photodiode;visible light communication;Decoding;Hardware;Lifting equipment;Light emitting diodes;Photodiodes;Protocols;Throughput}, doi={10.1109/GLOCOMW.2014.7063484}, ISSN={2166-0077}, month={Dec},} Connecting Networks of Toys and Smartphones with Visible Light Communication Giorgio Corbellini, Kaan Ak\u015fit , Stefan Mangold Stefan Schmid and Thomas R. Gross Publisher site Project site Manuscript Video Bibtex @ARTICLE{6852086, author={G. Corbellini and K. Aksit and S. Schmid and S. Mangold and T. R. Gross}, journal={IEEE Communications Magazine}, title={Connecting networks of toys and smartphones with visible light communication}, year={2014}, volume={52}, number={7}, pages={72-78}, keywords={light emitting diodes;optical communication;optical receivers;smart phones;LED;VLC systems;brightness;consumer electronics;illumination;light emitting diodes;light receivers;microcontrollers;public environment;residential environment;smartphones;toys;visible light communication;wireless communication interface;Cameras;Commercialization;Frequency measurement;Illumination;Light emitting diodes;Microcontrollers;Receivers;Smart phones;Transceivers}, doi={10.1109/MCOM.2014.6852086}, ISSN={0163-6804}, month={July},}","title":"2014"},{"location":"publications/#2013","text":"Dynamic exit pupil trackers for autostereoscopic displays Kaan Ak\u015fit , Hadi Baghsiahi, P. Surman, Selim \u04e6l\u00e7er, E. Willman, David R. Selviah, Sally Day and Hakan Urey Publisher site Project site Manuscript Video Bibtex @article{Aksit:13, author = {Kaan Ak\\c{s}it and Hadi Baghsiahi and Phil Surman and Selim \u04e6l\\c{c}er and Eero Willman and David R. Selviah and Sally Day and Hakan Urey}, journal = {Opt. Express}, keywords = {Displays; Optical systems; Optoelectronics; Laser beam shaping; Vision - binocular and stereopsis}, number = {12}, pages = {14331--14341}, publisher = {OSA}, title = {Dynamic exit pupil trackers for autostereoscopic displays}, volume = {21}, month = {Jun}, year = {2013}, url = {http://www.opticsexpress.org/abstract.cfm?URI=oe-21-12-14331}, doi = {10.1364/OE.21.014331}, abstract = {This paper describes the first demonstrations of two dynamic exit pupil (DEP) tracker techniques for autostereoscopic displays. The first DEP tracker forms an exit pupil pair for a single viewer in a defined space with low intraocular crosstalk using a pair of moving shutter glasses located within the optical system. A display prototype using the first DEP tracker is constructed from a pair of laser projectors, pupil-forming optics, moving shutter glasses at an intermediate pupil plane, an image relay lens, and a Gabor superlens based viewing screen. The left and right eye images are presented time-sequentially to a single viewer and seen as a 3D image without wearing glasses and allows the viewer to move within a region of 40 cm {\\texttimes} 20 cm in the lateral plane, and 30 cm along the axial axis. The second DEP optics can move the exit pupil location dynamically in a much larger 3D space by using a custom spatial light modulator (SLM) forming an array of shutters. Simultaneous control of multiple exit pupils in both lateral and axial axes is demonstrated for the first time and provides a viewing volume with an axial extent of 0.6{\\textminus}3 m from the screen and within a lateral viewing angle of {\\textpm} 20{\\textdegree} for multiple viewers. This system has acceptable crosstalk (\\&lt; 5\\%) between the stereo image pairs. In this novel version of the display the optical system is used as an advanced dynamic backlight for a liquid crystal display (LCD). This has advantages in terms of overall display size as there is no requirement for an intermediate image, and in image quality. This system has acceptable crosstalk (\\&lt; 5\\%) between the stereo image pairs.}, } Multi-view autostereoscopic projection display using rotating screen Spotlight on Optics Osman Eldes, Kaan Ak\u015fit and Hakan Urey Publisher site Project site Manuscript Video Bibtex @article{Eldes:13, author = {Osman Eldes and Kaan Ak\\c{s}it and Hakan Urey}, journal = {Opt. Express}, keywords = {Displays; Diffusers; Vision - binocular and stereopsis ; Autostereoscopic displays; Brightness; Fresnel lenses; Image registration; Pico projectors; Systems design}, number = {23}, pages = {29043--29054}, publisher = {OSA}, title = {Multi-view autostereoscopic projection display using rotating screen}, volume = {21}, month = {Nov}, year = {2013}, url = {http://www.osapublishing.org/oe/abstract.cfm?URI=oe-21-23-29043}, doi = {10.1364/OE.21.029043}, abstract = {A new technique for multi-view autostereoscopic projection display is proposed, and demonstrated. The technique uses two mobile projectors, a rotating retro-reflective diffuser screen, and a head-tracking camera. As two dynamic viewing slits are created at the viewer's position, the slits can track the position of the eyes by rotating the screen. The display allows a viewer to move approximately 700 mm along the horizontal axis, and 500 mm along the vertical axis with an average crosstalk below 5 \\%. Two screen prototypes with different diffusers have been tried, and they provide luminance levels of 60 Cd/m2, and 160 Cd/m2 within the viewing field.}, }","title":"2013"},{"location":"publications/#2012","text":"Portable 3D Laser Projector Using Mixed Polarization Technique Best 3D product award of International 3D Society (4th year) Kaan Ak\u015fit , Osman Elde\u015f, Selvan Viswanathan, Mark Freeman and Hakan Urey Publisher site Project site Manuscript Video Bibtex @ARTICLE{6297485, author={Aksit, Kaan and Eldes, Osman and Viswanathan, Selvan and Freeman, Mark O. and Urey, Hakan}, journal={Journal of Display Technology}, title={Portable 3D Laser Projector Using Mixed Polarization Technique}, year={2012}, volume={8}, number={10}, pages={582-589}, doi={10.1109/JDT.2012.2205664}}","title":"2012"},{"location":"publications/#2010","text":"Heart rate monitoring via remote photoplethysmography with motion artifacts reduction Giovanni Cennini, Jeremie Arguel, Kaan Ak\u015fit and Arno van Leest Publisher site Project site Manuscript Video Bibtex @article{Cennini:10, author = {Giovanni Cennini and Jeremie Arguel and Kaan Ak\\c{s}it and Arno van Leest}, journal = {Opt. Express}, keywords = {Medical optics instrumentation; Optical devices; Optical sensing and sensors}, number = {5}, pages = {4867--4875}, publisher = {OSA}, title = {Heart rate monitoring via remote photoplethysmography with motion artifacts reduction}, volume = {18}, month = {Mar}, year = {2010}, url = {http://www.opticsexpress.org/abstract.cfm?URI=oe-18-5-4867}, doi = {10.1364/OE.18.004867}, abstract = {In this paper, we present a novel photoplethysmographic device that operates remotely, i.e. not in contact with the skin. The device allows for real time measurements of heart rate with motion artifact reduction from a distance of a few centimeters up to several meters. High mobility of users is achieved in assessment of vital body signs, such as heart rate.}, }","title":"2010"},{"location":"seminars/","text":"Seminars \u00b6 We organize Virtual Reality and Computer Graphics group's (VECG's) weekly seminar series. VECG weekly seminar series is an exclusive event where we host experts across the industry and academia. Overall, seminars are a blend of internal and external presenters. Question If you are wondering how to get an invitation to the next seminar series, please do not hesitate to email Kaan Ak\u015fit . 2021 \u00b6 Wenzel Jakob (\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne) \u00b6 Details Date: 6th October 2021 Presenters: Wenzel Jakob , assistant professor at \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne Title: Differentiable Simulation of Light Watch: Recording (Password protected) Gordon Wetzstein (Stanford University) \u00b6 Details Date: 29th September 2021 Presenters: Gordon Wetzstein , Associate Professor at Stanford University Title: Towards Neural Signal Processing and Imaging Watch: Recording (Password protected) Anjul Patney (NVIDIA) \u00b6 Details Date: 22nd September 2021 Presenters: Anjul Patney , Principal Scientist at NVIDIA Title: Peripheral Perception & Pixels Watch: Recording (Password protected) Douglas Lanman (Facebook) \u00b6 Details Date: 15th September 2021 Presenters: Douglas Lanman , Director of Display Systems Research at Facebook Reality Labs, Affiliate Instructor at University of Washington Title: How to Pass the Visual Turing Test with AR/VR Displays Watch: Recording (Password protected) Sylvia Xueni Pan (Gold Smiths, University of London) \u00b6 Details Date: 8th September 2021 Presenters: Sylvia Xueni Pan, Lecturer in Graphics, Gold Smiths, University of London Title: Virtual Social Interaction in VR Watch: Recording (Password protected) Duygu Ceylan (Adobe) \u00b6 Details Date: 28th July 2021 Presenters: Duygu Ceylan , Senior Research Scientist, Adobe Title: Neural Dynamic Characters Watch: Recording (Password protected) Oliver Kingshott and Michael Fischer (University College London) \u00b6 Details Date: 21th July 2021 Presenters: Oliver Kingshott , MSc student at University College London Michael Fischer, PhD student at University College London Title: Lensless Learning Learning to Overfit Watch: Recording (Password protected) Yuta Itoh (Tokyo Institute of Technology) \u00b6 Details Date: 14th July 2021 Presenters: Yuta Itoh , Project Associate Professor at the University of Tokyo Title: Vision Augmentation: overwriting our visual world via computation Watch: Recording (Password protected) Kaan Ak\u015fit (University College London) \u00b6 Details Date: 7th July 2021 Presenters: Kaan Ak\u015fit , Associate Professor at University College London Title: Towards remote pixelless displays Watch: Recording (Password protected) Cengiz \u00d6ztireli (University of Cambridge, Google) \u00b6 Details Date: 28th June 2021 Presenters: Cengiz \u00d6ztireli , Associate Professor at University of Cambridge, Senior Researcher at Google Title: 3D Digital Reality - Modeling for Perception Watch: Recording (Password protected) Paul Linton (City, University of London) \u00b6 Details Date: 23rd June 2021 Presenters: Paul Linton , Research Fellow, Centre for Applied Vision Research, City, University of London Title: Size and Distance Perception for Virtual Reality Watch: Recording (Password protected) Luca Morreale and Lisa Izzouzi (University College London) \u00b6 Details Date: 16th June 2021 Presenters: Luca Morreale, PhD student at University College London Lisa Izzouzi, Phd student at University College London Title: - Interpretable Neural Surface Maps - Meaningful meetups in Virtual Reality Watch: Recording (Password protected) Rafa\u0142 Mantiuk (Cambridge University) \u00b6 Details Date: 9th June 2021 Presenter: Rafa\u0142 Mantiuk , Reader in Graphics and Displays at the University of Cambridge Title: Modelling the quality of high frame-rate graphics for adaptive refresh rate and resolution Watch: Recording (Password protected) Peter Shirley (NVIDIA) \u00b6 Details Date: 2nd June 2021 Presenter: Peter Shirley , Distinguished Research Scientist at NVIDIA Title: A tour of the rapidly moving target of computer graphics Watch: Recording (Password protected) David Walton and Rafel Kuffner dos Anjos (University College London) \u00b6 Details Date: 26th May 2021 Presenters: David Walton , Postdoctoral researcher at University College London Rafael Kuffner dos Anjos, Postdoctoral researcher at University College London Title: Beyond Blur: Ventral Metamers for Foveated Rendering Metameric Inpainting for Image Warping Watch: Recording (Password protected) Tobias Ritschel (University College London) \u00b6 Details Date: 19th May 2021 Presenters: Tobias Ritschel , Professor of Computer Graphics at University College London Title: Blue noise plots Watch: Not recorded Philip Henzler and David Griffiths (University College London) \u00b6 Details Date: 12th May 2021 Presenters: Philip Henzler, PhD student at University College London David Griffiths, PhD student at University College London Title: Generative Modelling of BRDF Textures from Flash Images 3D object detection without scene labels Watch: Recording (Password protected) Klara Brandst\u00e4tter and Felix Thiel (University College London) \u00b6 Details Date: 5th May 2021 Presenters: Klara Brandst\u00e4tter , PhD student at University College London Felix Thiel, PhD student at University College London Title: Creating Lively Interactive Populated Environments You have control. I have control Watch: Recording (Password protected) Victoria Rege and Alex Titterton (Graphcore) \u00b6 Details Date: 28th April 2021 Presenters: Victoria Rege , Director, Alliances & Strategic Partnerships at Graphcore Alex Titterton , Field Engineer at Graphcore (and former CERN Physicist) Title: Next in Machine Intelligence Watch: Recording (Password protected)","title":"Recent seminars"},{"location":"seminars/#seminars","text":"We organize Virtual Reality and Computer Graphics group's (VECG's) weekly seminar series. VECG weekly seminar series is an exclusive event where we host experts across the industry and academia. Overall, seminars are a blend of internal and external presenters. Question If you are wondering how to get an invitation to the next seminar series, please do not hesitate to email Kaan Ak\u015fit .","title":"Seminars"},{"location":"seminars/#2021","text":"","title":"2021"},{"location":"seminars/#wenzel-jakob-ecole-polytechnique-federale-de-lausanne","text":"Details Date: 6th October 2021 Presenters: Wenzel Jakob , assistant professor at \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne Title: Differentiable Simulation of Light Watch: Recording (Password protected)","title":"Wenzel Jakob (\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne)"},{"location":"seminars/#gordon-wetzstein-stanford-university","text":"Details Date: 29th September 2021 Presenters: Gordon Wetzstein , Associate Professor at Stanford University Title: Towards Neural Signal Processing and Imaging Watch: Recording (Password protected)","title":"Gordon Wetzstein (Stanford University)"},{"location":"seminars/#anjul-patney-nvidia","text":"Details Date: 22nd September 2021 Presenters: Anjul Patney , Principal Scientist at NVIDIA Title: Peripheral Perception & Pixels Watch: Recording (Password protected)","title":"Anjul Patney (NVIDIA)"},{"location":"seminars/#douglas-lanman-facebook","text":"Details Date: 15th September 2021 Presenters: Douglas Lanman , Director of Display Systems Research at Facebook Reality Labs, Affiliate Instructor at University of Washington Title: How to Pass the Visual Turing Test with AR/VR Displays Watch: Recording (Password protected)","title":"Douglas Lanman (Facebook)"},{"location":"seminars/#sylvia-xueni-pan-gold-smiths-university-of-london","text":"Details Date: 8th September 2021 Presenters: Sylvia Xueni Pan, Lecturer in Graphics, Gold Smiths, University of London Title: Virtual Social Interaction in VR Watch: Recording (Password protected)","title":"Sylvia Xueni Pan (Gold Smiths, University of London)"},{"location":"seminars/#duygu-ceylan-adobe","text":"Details Date: 28th July 2021 Presenters: Duygu Ceylan , Senior Research Scientist, Adobe Title: Neural Dynamic Characters Watch: Recording (Password protected)","title":"Duygu Ceylan (Adobe)"},{"location":"seminars/#oliver-kingshott-and-michael-fischer-university-college-london","text":"Details Date: 21th July 2021 Presenters: Oliver Kingshott , MSc student at University College London Michael Fischer, PhD student at University College London Title: Lensless Learning Learning to Overfit Watch: Recording (Password protected)","title":"Oliver Kingshott and Michael Fischer (University College London)"},{"location":"seminars/#yuta-itoh-tokyo-institute-of-technology","text":"Details Date: 14th July 2021 Presenters: Yuta Itoh , Project Associate Professor at the University of Tokyo Title: Vision Augmentation: overwriting our visual world via computation Watch: Recording (Password protected)","title":"Yuta Itoh (Tokyo Institute of Technology)"},{"location":"seminars/#kaan-aksit-university-college-london","text":"Details Date: 7th July 2021 Presenters: Kaan Ak\u015fit , Associate Professor at University College London Title: Towards remote pixelless displays Watch: Recording (Password protected)","title":"Kaan Ak\u015fit (University College London)"},{"location":"seminars/#cengiz-oztireli-university-of-cambridge-google","text":"Details Date: 28th June 2021 Presenters: Cengiz \u00d6ztireli , Associate Professor at University of Cambridge, Senior Researcher at Google Title: 3D Digital Reality - Modeling for Perception Watch: Recording (Password protected)","title":"Cengiz \u00d6ztireli (University of Cambridge, Google)"},{"location":"seminars/#paul-linton-city-university-of-london","text":"Details Date: 23rd June 2021 Presenters: Paul Linton , Research Fellow, Centre for Applied Vision Research, City, University of London Title: Size and Distance Perception for Virtual Reality Watch: Recording (Password protected)","title":"Paul Linton (City, University of London)"},{"location":"seminars/#luca-morreale-and-lisa-izzouzi-university-college-london","text":"Details Date: 16th June 2021 Presenters: Luca Morreale, PhD student at University College London Lisa Izzouzi, Phd student at University College London Title: - Interpretable Neural Surface Maps - Meaningful meetups in Virtual Reality Watch: Recording (Password protected)","title":"Luca Morreale and Lisa Izzouzi (University College London)"},{"location":"seminars/#rafa-mantiuk-cambridge-university","text":"Details Date: 9th June 2021 Presenter: Rafa\u0142 Mantiuk , Reader in Graphics and Displays at the University of Cambridge Title: Modelling the quality of high frame-rate graphics for adaptive refresh rate and resolution Watch: Recording (Password protected)","title":"Rafa\u0142 Mantiuk (Cambridge University)"},{"location":"seminars/#peter-shirley-nvidia","text":"Details Date: 2nd June 2021 Presenter: Peter Shirley , Distinguished Research Scientist at NVIDIA Title: A tour of the rapidly moving target of computer graphics Watch: Recording (Password protected)","title":"Peter Shirley (NVIDIA)"},{"location":"seminars/#david-walton-and-rafel-kuffner-dos-anjos-university-college-london","text":"Details Date: 26th May 2021 Presenters: David Walton , Postdoctoral researcher at University College London Rafael Kuffner dos Anjos, Postdoctoral researcher at University College London Title: Beyond Blur: Ventral Metamers for Foveated Rendering Metameric Inpainting for Image Warping Watch: Recording (Password protected)","title":"David Walton and Rafel Kuffner dos Anjos (University College London)"},{"location":"seminars/#tobias-ritschel-university-college-london","text":"Details Date: 19th May 2021 Presenters: Tobias Ritschel , Professor of Computer Graphics at University College London Title: Blue noise plots Watch: Not recorded","title":"Tobias Ritschel (University College London)"},{"location":"seminars/#philip-henzler-and-david-griffiths-university-college-london","text":"Details Date: 12th May 2021 Presenters: Philip Henzler, PhD student at University College London David Griffiths, PhD student at University College London Title: Generative Modelling of BRDF Textures from Flash Images 3D object detection without scene labels Watch: Recording (Password protected)","title":"Philip Henzler and David Griffiths (University College London)"},{"location":"seminars/#klara-brandstatter-and-felix-thiel-university-college-london","text":"Details Date: 5th May 2021 Presenters: Klara Brandst\u00e4tter , PhD student at University College London Felix Thiel, PhD student at University College London Title: Creating Lively Interactive Populated Environments You have control. I have control Watch: Recording (Password protected)","title":"Klara Brandst\u00e4tter and Felix Thiel (University College London)"},{"location":"seminars/#victoria-rege-and-alex-titterton-graphcore","text":"Details Date: 28th April 2021 Presenters: Victoria Rege , Director, Alliances & Strategic Partnerships at Graphcore Alex Titterton , Field Engineer at Graphcore (and former CERN Physicist) Title: Next in Machine Intelligence Watch: Recording (Password protected)","title":"Victoria Rege and Alex Titterton (Graphcore)"},{"location":"teaching/","text":"","title":"Lectures"},{"location":"teaching/comp0160_perception_and_interfaces/","text":"","title":"COMP0160 Perception and Interfaces"},{"location":"timeline/","text":"Timeline \u00b6 2021 \u00b6 October \u00b6 1 October 2021 \u00b6 Oliver Kingshott joined University College London's computer science department as a PhD student. He is now a member of Computational light laboratory and Virtual Reality and Computer Graphics group . His PhD studies will be supervised by Kaan Ak\u015fit and Tobias Ritschel . February \u00b6 18 February 2021 \u00b6 We appear on UCL news for receiving UCL-Osaka university strategic partnership fund. January \u00b6 4 January 2021 \u00b6 Kaan Ak\u015fit joined University College London's computer science department as an Associate Professor. He is now part of the Virtual Reality and Computer Graphics group , and he leads the Computational light laboratory . 2020 \u00b6 November \u00b6 17 November 2020 \u00b6 Kaan Ak\u015fit and Jason Orlosky They have been granted UCL-Osaka University Strategic Parner Funds funds. This award is worth 10000 GBP . The title of our submission is Development of a joint Telelife technology seminar using virtual reality . August \u00b6 1 August 2020 \u00b6 Kaan Ak\u015fit has left his Senior Research scientist position at NVIDIA in the US, and accepted to join University College London's computer science department as an Associate Professor.","title":"Recent news"},{"location":"timeline/#timeline","text":"","title":"Timeline"},{"location":"timeline/#2021","text":"","title":"2021"},{"location":"timeline/#october","text":"","title":"October"},{"location":"timeline/#1-october-2021","text":"Oliver Kingshott joined University College London's computer science department as a PhD student. He is now a member of Computational light laboratory and Virtual Reality and Computer Graphics group . His PhD studies will be supervised by Kaan Ak\u015fit and Tobias Ritschel .","title":"1 October 2021"},{"location":"timeline/#february","text":"","title":"February"},{"location":"timeline/#18-february-2021","text":"We appear on UCL news for receiving UCL-Osaka university strategic partnership fund.","title":"18 February 2021"},{"location":"timeline/#january","text":"","title":"January"},{"location":"timeline/#4-january-2021","text":"Kaan Ak\u015fit joined University College London's computer science department as an Associate Professor. He is now part of the Virtual Reality and Computer Graphics group , and he leads the Computational light laboratory .","title":"4 January 2021"},{"location":"timeline/#2020","text":"","title":"2020"},{"location":"timeline/#november","text":"","title":"November"},{"location":"timeline/#17-november-2020","text":"Kaan Ak\u015fit and Jason Orlosky They have been granted UCL-Osaka University Strategic Parner Funds funds. This award is worth 10000 GBP . The title of our submission is Development of a joint Telelife technology seminar using virtual reality .","title":"17 November 2020"},{"location":"timeline/#august","text":"","title":"August"},{"location":"timeline/#1-august-2020","text":"Kaan Ak\u015fit has left his Senior Research scientist position at NVIDIA in the US, and accepted to join University College London's computer science department as an Associate Professor.","title":"1 August 2020"}]}